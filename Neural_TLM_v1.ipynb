{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural_TLM_v1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smallcats/TopicalLanguageModels/blob/master/Neural_TLM_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFAvBLHfcmrD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "from keras.layers import Input, Embedding, LSTM, Dense, Flatten, Lambda, Concatenate, Reshape, Dot\n",
        "from keras.models import Model\n",
        "import keras.backend as K\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjRlyo009h4g",
        "colab_type": "code",
        "outputId": "1b6599dd-a71f-4cec-9e2d-a3a6b5574641",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stopwords = stopwords.words('english')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-04uRZH79iY5",
        "colab_type": "code",
        "outputId": "2d9795a8-1e1b-4cb6-d8a3-24f514695249",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "thurs = nltk.corpus.gutenberg.raw('chesterton-thursday.txt')\n",
        "len(thurs)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "320525"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQN8KBSq-AnY",
        "colab_type": "code",
        "outputId": "b16ba783-8c8a-46d1-a5ba-ed3ff1ccf858",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "thurs = thurs.split('\\n\\n')\n",
        "len(thurs)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1304"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "15cbbb69-50b4-4682-8b57-813e5f398a2b",
        "id": "TuZus5wfdL35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plt.hist([len(d) for d in thurs], bins=20)\n",
        "plt.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNRLjhxP-Ai3",
        "colab_type": "code",
        "outputId": "56c9fe03-619e-4ed8-8442-ff5366d30ab9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plt.hist([len(d) for d in thurs if len(d)<200], bins=20)\n",
        "plt.show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nPy5lZitkLivvAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDP5oF9D-AeS",
        "colab_type": "code",
        "outputId": "6143b08b-36c6-4ddf-a5f8-9addd4390b02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len([d for d in thurs if len(d)>75])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "876"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoJra3Vad-O_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "thurs = [d for d in thurs if len(d)>75]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_5mQk_Wd9-D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "thurs = [' '.join(d.split()) for d in thurs]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irGTcbyGd9r_",
        "colab_type": "code",
        "outputId": "04087b20-defd-41ff-f7bf-1a30436259bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "thurs[10]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\"Nonsense!\" said Gregory, who was very rational when anyone else attempted paradox. \"Why do all the clerks and navvies in the railway trains look so sad and tired, so very sad and tired? I will tell you. It is because they know that the train is going right. It is because they know that whatever place they have taken a ticket for that place they will reach. It is because after they have passed Sloane Square they know that the next station must be Victoria, and nothing but Victoria. Oh, their wild rapture! oh, their eyes like stars and their souls again in Eden, if the next station were unaccountably Baker Street!\"'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krbyOqDQbUXl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "thurs_sent = '\\n'.join(thurs)\n",
        "thurs_sent = nltk.tokenize.sent_tokenize(thurs_sent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBfZrNBFbO1s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def doc2words(doc):\n",
        "  words = nltk.tokenize.word_tokenize(doc)\n",
        "  words = [w.lower() for w in words if w.isalpha()]\n",
        "  words = [w for w in words if (len(w)>1) and (w not in stopwords)]\n",
        "  return words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jw7qG968d4Im",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "thurs_sent = [doc2words(s) for s in thurs_sent]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pI1pCVKGaXdx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emb = Word2Vec(thurs_sent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OjwJPY8ahTl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "972bef28-fbb9-4a30-c9b4-653b40668857"
      },
      "source": [
        "emb.wv.similar_by_word('door')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('street', 0.9957243800163269),\n",
              " ('colonel', 0.9956061244010925),\n",
              " ('light', 0.9955676794052124),\n",
              " ('like', 0.9955669641494751),\n",
              " ('upon', 0.99552321434021),\n",
              " ('one', 0.9954705238342285),\n",
              " ('white', 0.9954642653465271),\n",
              " ('said', 0.9954573512077332),\n",
              " ('syme', 0.9954417943954468),\n",
              " ('man', 0.9954401254653931)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiRzBAaCgcDV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "54f3c511-b263-44bf-bb6b-4e9c0141446c"
      },
      "source": [
        "'door' in emb.wv.vocab"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMHtfDD6adq5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "we = {w:emb.wv.get_vector(w) for w in emb.wv.vocab}\n",
        "vec_size = emb.vector_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Kfc5xpJd9E2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ye93C1mdrQR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def multiples(it, n):\n",
        "  for k in range(len(it)-n+1):\n",
        "    yield it[k:k+n]\n",
        "\n",
        "class TopicalLanguageModel:\n",
        "  \"\"\"\n",
        "  Neural Topical Language Model.\n",
        "\n",
        "  attributes:\n",
        "  num_topics :: int : number of topics\n",
        "  doc2words :: function : takes a document as a string and returns words\n",
        "  word_embedding :: dict : keys are words, values are vectors\n",
        "  vector_size :: int : the dimension of embedding vectors\n",
        "  lm_length :: int : length of tokens taken into account for character-level language models \n",
        "  \"\"\"\n",
        "  def __init__(self, num_topics, word_embedding, vector_size, doc2words, lm_length=30, tokens='abcdefghijklmnopqrstuvwxyz '):\n",
        "    self.num_topics = num_topics\n",
        "    self.doc2words = doc2words\n",
        "    self.word_embedding = word_embedding\n",
        "    self.vector_size = vector_size\n",
        "    self.lm_length = lm_length\n",
        "\n",
        "    self.tokens = ['<start>', '<end>', '<unk>'] + list(tokens)\n",
        "    token_idx = [(t, k+3) for k,t in enumerate(list(tokens))] # 0 = start, 1 = end, 2 = unknown\n",
        "    self.token2idx = {t:k for t,k in token_idx}\n",
        "    self.idx2token = {k:t for t,k in token_idx}\n",
        "\n",
        "    self.num_tokens = len(tokens)+3\n",
        "\n",
        "  def generate_train_data(self, documents, batch_size=20): #fix documents inconsistency (list of document strings or list of list of words)\n",
        "    while True:\n",
        "      docs = np.random.choice(documents, size=batch_size)\n",
        "      finish_idxs = [np.random.randint(0, len(d)+1) for d in docs]\n",
        "      \n",
        "      x_topic = np.array([np.array([self.word_embedding[w] for w in self.doc2words(d) if w in self.word_embedding]).mean(axis=0) for d in docs])\n",
        "\n",
        "      lm_docs = [d[max(i-self.lm_length,0):i] for i, d in zip(finish_idxs, docs)]\n",
        "      x_lm = np.array([[0]*(self.lm_length - len(d))+[self.token2idx.get(t,2) for t in d] for d in lm_docs])\n",
        "\n",
        "      y = np.array([self.token2idx[d[i]] if i != len(d) else 1 for i,d in zip(finish_idxs, docs)])\n",
        "      \n",
        "      yield [x_topic, x_lm], y\n",
        "\n",
        "  def fit(self, documents, batch_size=20, epochs=10, verbose=0):\n",
        "    \"\"\"\n",
        "    fit(self, documents)\n",
        "\n",
        "    documents should be a list of sentences, and a sentence a list of words.\n",
        "    \"\"\"\n",
        "    x_topic = Input(shape=(self.vector_size,))\n",
        "    h_topic = Dense(self.vector_size, activation='relu')(x_topic)\n",
        "    out_topic = Dense(self.num_topics, activation='softmax')(h_topic)\n",
        "    self.topic_model = Model(inputs=[x_topic], outputs=out_topic)\n",
        "    \n",
        "    self.language_models = []\n",
        "    for k in range(self.num_topics):\n",
        "      x_lm = Input(shape=(self.lm_length,), dtype='int32')\n",
        "      h_lm = Embedding(input_dim=self.num_tokens, output_dim=self.num_tokens//2, input_length=self.lm_length)(x_lm)\n",
        "      h_lm = LSTM(self.num_tokens)(h_lm)\n",
        "      out_lm = Dense(self.num_tokens, activation='softmax')(h_lm)\n",
        "      lm = Model(inputs=[x_lm], outputs=out_lm)\n",
        "      self.language_models.append(lm)\n",
        "\n",
        "    train_topic_input = Input(shape=(self.vector_size,))\n",
        "    train_lm_input = Input(shape=(self.lm_length,), dtype='int32')\n",
        "\n",
        "    lm_outputs = [Reshape(target_shape=(self.num_tokens, 1))(lm(train_lm_input)) for lm in self.language_models]\n",
        "    lm_outputs = Concatenate(axis=2)(lm_outputs)\n",
        "    topic_mix = self.topic_model(train_topic_input)\n",
        "\n",
        "    out = Dot(axes=(2, 1))([lm_outputs, topic_mix])\n",
        "\n",
        "    self.train_model = Model(inputs=[train_topic_input, train_lm_input], outputs=out)\n",
        "    self.train_model.compile('adam', 'sparse_categorical_crossentropy')\n",
        "\n",
        "    self.train_hist = self.train_model.fit_generator(self.generate_train_data(documents, batch_size=batch_size), \n",
        "                                                     steps_per_epoch=len(documents)//batch_size, \n",
        "                                                     epochs=epochs)\n",
        "\n",
        "  def predict(self, init_doc, topic, method='sample'):\n",
        "    idx_doc = [self.token2idx[c] for c in init_doc]\n",
        "    dlen = len(idx_doc)\n",
        "    if dlen < self.lm_length:\n",
        "      idx_doc = [0]*(self.lm_length-dlen)+idx_doc\n",
        "    elif dlen > self.lm_length:\n",
        "      idx_doc = idx_doc[-self.lm_length:]\n",
        "\n",
        "    probs = self.language_models[topic].predict(np.array(idx_doc).reshape((1,-1)))[0]\n",
        "\n",
        "    if method == 'sample':\n",
        "      return np.random.choice(self.tokens, p=probs)\n",
        "\n",
        "    elif method == 'max':\n",
        "      return self.tokens[np.argmax(probs)]\n",
        "\n",
        "    elif method == 'distribution':\n",
        "      return probs\n",
        "\n",
        "    else:\n",
        "      raise ValueError('Unknown method.')\n",
        "\n",
        "  def rollout(self, init_doc, topic, method='monte-carlo', maxlen=100):\n",
        "    pred_method = 'sample' if method=='monte-carlo' else 'max' if method=='greedy' else ''\n",
        "    \n",
        "    for k in range(maxlen-len(init_doc)):\n",
        "      predicted = self.predict(init_doc, topic, pred_method)\n",
        "      if predicted == '<end>': break\n",
        "      init_doc = init_doc + predicted\n",
        "\n",
        "    return init_doc\n",
        "\n",
        "  def get_topics(self, doc):\n",
        "    words = self.doc2words(doc)\n",
        "    embedding = np.array([self.word_embedding[w] for w in words if w in self.word_embedding]).mean(axis=0)\n",
        "    return self.topic_model.predict(embedding.reshape((-1,1)))[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcTAymOvfDSi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tlm = TopicalLanguageModel(num_topics=2, word_embedding=emb, vector_size=vec_size, doc2words=doc2words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H65lttW6g8ye",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "392756db-c8a2-455d-c457-e3ed246e6b26"
      },
      "source": [
        "tlm.fit(thurs, batch_size=5)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:35: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:35: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-4cad669f9bf5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthurs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-62-b01793a7a393>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, documents, batch_size, epochs, verbose)\u001b[0m\n\u001b[1;32m     76\u001b[0m     self.train_hist = self.train_model.fit_generator(self.generate_train_data(documents, batch_size=batch_size), \n\u001b[1;32m     77\u001b[0m                                                      \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                                                      epochs=epochs)\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_doc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sample'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m                     \u001b[0;34m\"`use_multiprocessing=False, workers > 1`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m                     \"For more information see issue #1638.\")\n\u001b[0;32m--> 733\u001b[0;31m             \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                     \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m                     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwrap_exception\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_helper_reraises_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mnext_sample\u001b[0;34m(uid)\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0mThe\u001b[0m \u001b[0mnext\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mof\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0muid\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m     \"\"\"\n\u001b[0;32m--> 641\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_SHARED_SEQUENCES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-62-b01793a7a393>\u001b[0m in \u001b[0;36mgenerate_train_data\u001b[0;34m(self, documents, batch_size)\u001b[0m\n\u001b[1;32m     38\u001b[0m       \u001b[0mx_lm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_length\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken2idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlm_docs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m       \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinish_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m       \u001b[0;32myield\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx_topic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_lm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-62-b01793a7a393>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     38\u001b[0m       \u001b[0mx_lm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_length\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken2idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlm_docs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m       \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinish_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m       \u001b[0;32myield\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx_topic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_lm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: ','"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJtqCYehlTaR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "34d73422-a4ad-4317-e948-d8a64380acb3"
      },
      "source": [
        "a = 'abcd'\n",
        "a[4] if 4 != len(a) else 1"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PG_p8J4O6omM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m59-hFfjlQys",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}