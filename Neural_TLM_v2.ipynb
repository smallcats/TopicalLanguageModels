{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural_TLM_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smallcats/TopicalLanguageModels/blob/master/Neural_TLM_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTWYvbXC68CH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Flatten, Lambda, Concatenate, Reshape, Dot, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WP5TY8t97Gk1",
        "colab_type": "code",
        "outputId": "6742917d-b32c-48e1-d914-517bb1c5be41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stopwords = stopwords.words('english')"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4z-eaM57HjV",
        "colab_type": "code",
        "outputId": "62a4bece-8390-4377-a55b-500a9f51e37f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        }
      },
      "source": [
        "nltk.corpus.gutenberg.fileids()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['austen-emma.txt',\n",
              " 'austen-persuasion.txt',\n",
              " 'austen-sense.txt',\n",
              " 'bible-kjv.txt',\n",
              " 'blake-poems.txt',\n",
              " 'bryant-stories.txt',\n",
              " 'burgess-busterbrown.txt',\n",
              " 'carroll-alice.txt',\n",
              " 'chesterton-ball.txt',\n",
              " 'chesterton-brown.txt',\n",
              " 'chesterton-thursday.txt',\n",
              " 'edgeworth-parents.txt',\n",
              " 'melville-moby_dick.txt',\n",
              " 'milton-paradise.txt',\n",
              " 'shakespeare-caesar.txt',\n",
              " 'shakespeare-hamlet.txt',\n",
              " 'shakespeare-macbeth.txt',\n",
              " 'whitman-leaves.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sq8Cw9Ct7MyJ",
        "colab_type": "code",
        "outputId": "b58622f0-e259-4195-ffd7-d2cac901da41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "thurs = nltk.corpus.gutenberg.raw('chesterton-thursday.txt')\n",
        "leaves = nltk.corpus.gutenberg.raw('whitman-leaves.txt')\n",
        "len(thurs), len(leaves)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(320525, 711215)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRgEYnfL8BYe",
        "colab_type": "code",
        "outputId": "b395a679-3043-433a-91f6-fe18dde49ce2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "thurs = thurs.split('\\n\\n')\n",
        "leaves = leaves.split('\\n\\n')\n",
        "len(thurs), len(leaves)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1304, 2867)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTUUBp5F8JRe",
        "colab_type": "code",
        "outputId": "11217328-1426-4744-e8de-64b3e9072d3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plt.hist([len(d) for d in thurs], bins=np.linspace(0,1000,20), histtype='step')\n",
        "plt.hist([len(d) for d in leaves], bins=np.linspace(0,1000,20), histtype='step')\n",
        "plt.show()"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n2QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHMfhNO48MBV",
        "colab_type": "code",
        "outputId": "fa8bccc8-d66c-4475-b7fe-5dd36ffbce0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plt.hist([len(d) for d in thurs if len(d)<200], bins=np.linspace(0,200,20), histtype='step')\n",
        "plt.hist([len(d) for d in leaves if len(d)<200], bins=np.linspace(0,200,20), histtype='step')\n",
        "plt.show()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nY76MAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2te1F_gZ8QDo",
        "colab_type": "code",
        "outputId": "9b652b5e-2086-4aee-dfbb-d6bdf5690d72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len([d for d in thurs if len(d)>50]), len([d for d in leaves if len(d)>50])"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1084, 2054)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsWKSbWc8TvV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "texts = [d for d in thurs if len(d)>50]+[d for d in leaves if len(d)>50]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTaaGkuo8rIZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "texts = [' '.join(d.split()) for d in texts]\n",
        "texts = [d for d in texts if len(d)>50]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mrLnAlr8yPL",
        "colab_type": "code",
        "outputId": "86d90386-20af-4870-f428-6e1b3658196e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "texts[10]"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The man with the meek blue eyes and the pale, pointed beard endured these thunders with a certain submissive solemnity. The third party of the group, Gregory's sister Rosamond, who had her brother's braids of red hair, but a kindlier face underneath them, laughed with such mixture of admiration and disapproval as she gave commonly to the family oracle.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-TfLP3P84cQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sents = '\\n'.join(texts)\n",
        "sents = nltk.tokenize.sent_tokenize(sents)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dMNjtnq85HH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def doc2words(doc):\n",
        "  words = nltk.tokenize.word_tokenize(doc)\n",
        "  words = [w.lower() for w in words if w.isalpha()]\n",
        "  words = [w for w in words if (len(w)>1) and (w not in stopwords)]\n",
        "  return words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXikDLMM87wU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sents = [doc2words(s) for s in sents]\n",
        "sents = [s for s in sents if len(s)>0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCN8LPVL8_eP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emb = Word2Vec(sents)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aMbaP199B5e",
        "colab_type": "code",
        "outputId": "71815e3e-8d1e-40b4-ca8e-17af0895ed11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "source": [
        "emb.wv.similar_by_word('door')"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('time', 0.9998369216918945),\n",
              " ('also', 0.9998355507850647),\n",
              " ('back', 0.9998297691345215),\n",
              " ('would', 0.9998278617858887),\n",
              " ('heart', 0.999823272228241),\n",
              " ('great', 0.9998228549957275),\n",
              " ('stand', 0.99982088804245),\n",
              " ('without', 0.9998207092285156),\n",
              " ('blood', 0.999820351600647),\n",
              " ('strong', 0.999820351600647)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqI-Ylrx9GId",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "we = {w:emb.wv.get_vector(w) for w in emb.wv.vocab}\n",
        "vec_size = emb.vector_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJjpboC_9JI6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def multiples(it, n):\n",
        "  for k in range(len(it)-n+1):\n",
        "    yield it[k:k+n]\n",
        "\n",
        "class TLMDataGen:\n",
        "  def __init__(self, documents, word_embedding, doc2words, lm_length, token2idx, vector_size,\n",
        "               valid_num=None, valid_frac=None, batch_size=20, valid_batch_size=100):\n",
        "    if valid_num is None and valid_frac is None:\n",
        "      if len(documents) < 1000:\n",
        "        valid_num = int(0.1*len(documents))\n",
        "      else:\n",
        "        valid_num = 100\n",
        "    elif valid_num is None:\n",
        "      valid_num = int(valid_frac*len(documents))\n",
        "\n",
        "    perm = np.random.permutation(len(documents))\n",
        "    self.train_idxs = perm[:valid_num]\n",
        "    self.valid_idxs = perm[valid_num:]\n",
        "\n",
        "    rep = {'<':'*', '>':'*'}\n",
        "    rep = {re.escape(k):v for k,v in rep.items()}\n",
        "    pattern = re.compile('|'.join(rep.keys()))\n",
        "\n",
        "    self.documents = ['<'*lm_length+pattern.sub(lambda m: rep[re.escape(m.group(0))], d)+'>' for d in documents]\n",
        "    self.batch_size = batch_size\n",
        "    self.valid_batch_size = valid_batch_size\n",
        "    self.word_embedding = word_embedding\n",
        "    self.doc2words = doc2words\n",
        "    self.lm_length = lm_length\n",
        "    self.token2idx = token2idx\n",
        "    self.vector_size = vector_size\n",
        "\n",
        "  def get_train_batch(self):\n",
        "    doc_idxs = np.random.choice(self.train_idxs, size=self.batch_size)\n",
        "    docs = [self.documents[k] for k in doc_idxs]\n",
        "    finish_idxs = [np.random.randint(self.lm_length+1, len(d)) for d in docs]\n",
        "    \n",
        "    x_topic = [np.array([self.word_embedding[w] for w in self.doc2words(d) if w in self.word_embedding]) for d in docs]\n",
        "    x_topic = [d if d.shape != (0,) else np.zeros((1, self.vector_size)) for d in x_topic]\n",
        "    x_topic = [d.mean(axis=0) for d in x_topic]\n",
        "    x_topic = np.array(x_topic)\n",
        "\n",
        "    lm_docs = [d[i-self.lm_length-1:i] for i, d in zip(finish_idxs, docs)]\n",
        "    lm_docs = np.array([[self.token2idx.get(t,2) for t in d] for d in lm_docs])\n",
        "\n",
        "    return [x_topic, lm_docs[:,:-1]], lm_docs[:,1:]\n",
        "\n",
        "  def get_valid_batch(self):\n",
        "    doc_idxs = np.random.choice(self.valid_idxs, size=self.valid_batch_size)\n",
        "    docs = [self.documents[k] for k in doc_idxs]\n",
        "    finish_idxs = [np.random.randint(self.lm_length+1, len(d)) for d in docs]\n",
        "    \n",
        "    x_topic = [np.array([self.word_embedding[w] for w in self.doc2words(d) if w in self.word_embedding]) for d in docs]\n",
        "    x_topic = [d if d.shape != (0,) else np.zeros((1, self.vector_size)) for d in x_topic]\n",
        "    x_topic = [d.mean(axis=0) for d in x_topic]\n",
        "    x_topic = np.array(x_topic)\n",
        "\n",
        "    lm_docs = [d[i-self.lm_length-1:i] for i, d in zip(finish_idxs, docs)]\n",
        "    lm_docs = np.array([[self.token2idx.get(t,2) for t in d] for d in lm_docs])\n",
        "\n",
        "    return [x_topic, lm_docs[:,:-1]], lm_docs[:,1:]\n",
        "\n",
        "\n",
        "class TopicalLanguageModel:\n",
        "  \"\"\"\n",
        "  Neural Topical Language Model.\n",
        "\n",
        "  attributes:\n",
        "  num_topics :: int : number of topics\n",
        "  doc2words :: function : takes a document as a string and returns words\n",
        "  word_embedding :: dict : keys are words, values are vectors\n",
        "  vector_size :: int : the dimension of embedding vectors\n",
        "  lm_length :: int : length of tokens taken into account for character-level language models \n",
        "  \"\"\"\n",
        "  def __init__(self, num_topics, word_embedding, vector_size, doc2words, lm_length=30, tokens='abcdefghijklmnopqrstuvwxyz '):\n",
        "    self.num_topics = num_topics\n",
        "    self.doc2words = doc2words\n",
        "    self.word_embedding = word_embedding\n",
        "    self.vector_size = vector_size\n",
        "    self.lm_length = lm_length\n",
        "\n",
        "    self.tokens = ['<', '>', '*'] + list(tokens)\n",
        "    token_idx = [(t, k+3) for k,t in enumerate(list(tokens))]+[('<',0),('>',1),('*',2)]\n",
        "    self.token2idx = {t:k for t,k in token_idx}\n",
        "    self.idx2token = {k:t for t,k in token_idx}\n",
        "\n",
        "    self.num_tokens = len(tokens)+3\n",
        "\n",
        "  def fit(self, documents, batch_size=20, epochs=10, verbose=0, examples_per_epoch=5, example_epoch_skip=1, num_validation_samples=5):\n",
        "    \"\"\"\n",
        "    fit(self, documents)\n",
        "\n",
        "    documents should be a list of sentences, and a sentence a list of words.\n",
        "    \"\"\"\n",
        "    x_topic = Input(shape=(self.vector_size,))\n",
        "    h_topic = Dense(self.vector_size, activation='relu')(x_topic)\n",
        "    out_topic = Dense(self.num_topics, activation='softmax')(h_topic)\n",
        "    self.topic_model = Model(inputs=[x_topic], outputs=out_topic)\n",
        "    \n",
        "    self.language_models = []\n",
        "    for k in range(self.num_topics):\n",
        "      x_lm = Input(shape=(self.lm_length,), dtype='int32')\n",
        "      h_lm = Embedding(input_dim=self.num_tokens, output_dim=self.num_tokens//2, input_length=self.lm_length)(x_lm)\n",
        "      h_lm = LSTM(self.num_tokens, return_sequences=True)(h_lm)\n",
        "      h_lm = LSTM(self.num_tokens, return_sequences=True)(h_lm)\n",
        "      out_lm = TimeDistributed(Dense(self.num_tokens, activation='softmax'))(h_lm)\n",
        "      lm = Model(inputs=[x_lm], outputs=out_lm)\n",
        "      self.language_models.append(lm)\n",
        "\n",
        "    train_topic_input = Input(shape=(self.vector_size,))\n",
        "    train_lm_input = Input(shape=(self.lm_length,), dtype='int32')\n",
        "\n",
        "    lm_outputs = [Reshape(target_shape=(self.lm_length, self.num_tokens, 1))(lm(train_lm_input)) for lm in self.language_models]\n",
        "    lm_outputs = Concatenate(axis=3)(lm_outputs)\n",
        "    topic_mix = self.topic_model(train_topic_input)\n",
        "\n",
        "    out = Dot(axes=(3, 1), name='mixed_out')([lm_outputs, topic_mix])\n",
        "\n",
        "    self.train_model = Model(inputs=[train_topic_input, train_lm_input], outputs=[out, topic_mix])\n",
        "\n",
        "    def mix_loss(y_true, y_pred):\n",
        "      eps = 1e-10\n",
        "      mean = K.mean(y_pred, axis=0)\n",
        "\n",
        "      mean_nentropy = K.mean(K.sum(y_pred*K.log(y_pred+eps), axis=1))\n",
        "      nentropy_mean = K.sum(mean*K.log(mean+eps))\n",
        "      return nentropy_mean - mean_nentropy\n",
        "      \n",
        "\n",
        "    self.train_model.compile('adam', ['sparse_categorical_crossentropy', mix_loss])\n",
        "\n",
        "    data_generator = TLMDataGen(documents, word_embedding=self.word_embedding, \n",
        "                                doc2words=self.doc2words, \n",
        "                                lm_length=self.lm_length, \n",
        "                                token2idx=self.token2idx,\n",
        "                                vector_size=self.vector_size,\n",
        "                                batch_size = batch_size)\n",
        "\n",
        "    steps_per_epoch = len(documents)//batch_size\n",
        "    self.logs = {'examples':[], 'val_loss':[], 'topic_fraction':[], 'topic_entropy':[]}\n",
        "    for epoch in range(epochs+1):\n",
        "\n",
        "      #examples\n",
        "      if epoch%example_epoch_skip == 0:\n",
        "        if verbose>1:\n",
        "          print('Getting examples')\n",
        "        for t in range(self.num_topics):\n",
        "          for k in range(examples_per_epoch):\n",
        "            self.logs['examples'].append((self.rollout('', topic=t, maxlen=100), t, epoch))\n",
        "\n",
        "      #validation loss\n",
        "      if verbose>1:\n",
        "        print('Running validation loss')\n",
        "      xs, y = data_generator.get_valid_batch()\n",
        "      losses = [self.train_model.test_on_batch(xs, [y, np.zeros((y.shape[0], self.num_topics))]) for k in range(num_validation_samples)]\n",
        "      self.logs['val_loss'].append(losses)\n",
        "\n",
        "      #topics\n",
        "      if verbose>1:\n",
        "        print('Getting topic distributions')\n",
        "      xs, y = data_generator.get_valid_batch()\n",
        "      x = xs[0]\n",
        "      topics = self.topic_model.predict(x)\n",
        "      entropies = (-topics*np.log(topics)).sum(axis=1)\n",
        "      mean_entropy = entropies.mean(axis=0)\n",
        "      std_entropy = entropies.std(axis=0)\n",
        "      mean_topics = topics.mean(axis=0)\n",
        "      self.logs['topic_fraction'].append(mean_topics)\n",
        "      self.logs['topic_entropy'].append((mean_entropy, std_entropy))\n",
        "\n",
        "      if epoch<epochs:\n",
        "        #train\n",
        "        if verbose>1:\n",
        "          print('Training')\n",
        "        avg_loss = 0\n",
        "        for step in range(steps_per_epoch):\n",
        "          xs, y = data_generator.get_train_batch()\n",
        "          avg_loss = (step/(step+1))*avg_loss + self.train_model.train_on_batch(xs, [y, np.zeros((y.shape[0], self.num_topics))])[0]/(step+1)\n",
        "        #print\n",
        "        if verbose > 0:\n",
        "          if epoch%example_epoch_skip == 0:\n",
        "            print('Epoch {0} - Avg train loss: {1:1.5f} - Sample: {2}'.format(epoch, avg_loss, self.logs['examples'][-1][0][:self.lm_length]))\n",
        "          else:\n",
        "            print('Epoch {0} - Avg train loss: {1}'.format(epoch, avg_loss))\n",
        "      \n",
        "  def predict(self, init_doc, topic, method='sample'):\n",
        "    idx_doc = [self.token2idx.get(c,2) for c in init_doc]\n",
        "    dlen = len(idx_doc)\n",
        "    if dlen < self.lm_length:\n",
        "      idx_doc = [0]*(self.lm_length-dlen)+idx_doc\n",
        "    elif dlen > self.lm_length:\n",
        "      idx_doc = idx_doc[-self.lm_length:]\n",
        "\n",
        "    probs = self.language_models[topic].predict(np.array(idx_doc).reshape((1,-1)))[0,-1,:]\n",
        "\n",
        "    if method == 'sample':\n",
        "      return np.random.choice(self.tokens, p=probs)\n",
        "\n",
        "    elif method == 'max':\n",
        "      return self.tokens[np.argmax(probs)]\n",
        "\n",
        "    elif method == 'distribution':\n",
        "      return probs\n",
        "\n",
        "    else:\n",
        "      raise ValueError('Unknown method.')\n",
        "\n",
        "  def rollout(self, init_doc, topic, method='monte-carlo', maxlen=100):\n",
        "    pred_method = 'sample' if method=='monte-carlo' else 'max' if method=='greedy' else ''\n",
        "    \n",
        "    for k in range(maxlen-len(init_doc)):\n",
        "      predicted = self.predict(init_doc, topic, pred_method)\n",
        "      if predicted == '>': break\n",
        "      init_doc = init_doc + predicted\n",
        "\n",
        "    return init_doc\n",
        "\n",
        "  def get_topics(self, doc):\n",
        "    words = self.doc2words(doc)\n",
        "    embedding = np.array([self.word_embedding[w] for w in words if w in self.word_embedding]).mean(axis=0)\n",
        "    return self.topic_model.predict(embedding.reshape((-1,1)))[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qp_HNWNg9fFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "K.clear_session()\n",
        "tlm = TopicalLanguageModel(num_topics=2, word_embedding=we, vector_size=vec_size, doc2words=doc2words, lm_length=40)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "af0a6c75-0f32-4fdc-d976-b1717082889c",
        "id": "077DbzPFZlkS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "tlm.fit(texts, epochs=300, verbose=1, example_epoch_skip=1, examples_per_epoch=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - Avg train loss: 2.85581 - Sample: ah<tepbk*zxthv<zvws*v\n",
            "Epoch 1 - Avg train loss: 2.53673 - Sample: <<<<<<<<<<<<<<<t<s<<a l da dhntoidtxri*i\n",
            "Epoch 2 - Avg train loss: 2.27183 - Sample: <<<<<<<<<<<<<<<< amvoesd  hsnrnibmt lcne\n",
            "Epoch 3 - Avg train loss: 2.02786 - Sample: <<<<<<<<<<*<ool rteae er rt neirroen yxt\n",
            "Epoch 4 - Avg train loss: 1.86551 - Sample: <<<<<<< ***o t th e teona\n",
            "Epoch 5 - Avg train loss: 1.72728 - Sample: <<<<  * e* *oosecrt *** keitlieraf rhe r\n",
            "Epoch 6 - Avg train loss: 1.67013 - Sample: <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<** s \n",
            "Epoch 7 - Avg train loss: 1.61294 - Sample: <<<<<<<<<<<<<* *y *rreas *kr* mhodoy iyn\n",
            "Epoch 8 - Avg train loss: 1.56757 - Sample: <<<<<<<<<<<<<<<<<<<<<<<<<<<<**etor byod \n",
            "Epoch 9 - Avg train loss: 1.52403 - Sample: <<<<<<<<<<<<<<<<<<<<<<** rad hl reiins* \n",
            "Epoch 10 - Avg train loss: 1.49257 - Sample: <<<<<<<<<<*oalut tot ot *ewanpite fhr ar\n",
            "Epoch 11 - Avg train loss: 1.48419 - Sample: <<< es meld noe to sared* *he ceneundf w\n",
            "Epoch 12 - Avg train loss: 1.44854 - Sample: <<<<<<<<<<<<<<<<*se yalne naf * jrithhat\n",
            "Epoch 13 - Avg train loss: 1.43150 - Sample: <<* *es nneser* do oagdelk seyure** fete\n",
            "Epoch 14 - Avg train loss: 1.41696 - Sample: <<<*or the hore* bonle* bo okd atokd in \n",
            "Epoch 15 - Avg train loss: 1.38648 - Sample: *fimh thed* ** sdere* neurs *ot soed * t\n",
            "Epoch 16 - Avg train loss: 1.36845 - Sample: <<<<* whanou rant* *hemy *ressiws nermon\n",
            "Epoch 17 - Avg train loss: 1.35535 - Sample: <* *f axio the ned peett* sifor* nathes \n",
            "Epoch 18 - Avg train loss: 1.34249 - Sample: *eaddtains* the vord photite ows heyisec\n",
            "Epoch 19 - Avg train loss: 1.32235 - Sample: <<<<<<<<<<<<<<<<<<<<<<hoid od tawkantoar\n",
            "Epoch 20 - Avg train loss: 1.33758 - Sample: <<<<<<<** arl dyol thoren** *hends thre \n",
            "Epoch 21 - Avg train loss: 1.31011 - Sample: <<<<<<<*oage tho thye ou the sinc clye h\n",
            "Epoch 22 - Avg train loss: 1.30016 - Sample: <*eave of nid the seacinkrat setled ifwe\n",
            "Epoch 23 - Avg train loss: 1.28053 - Sample: <<<<<<<<<<<<<* abe yue kyat bet on thle \n",
            "Epoch 24 - Avg train loss: 1.28161 - Sample: * * ** femar to th on theveu* *he wrasin\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 25 - Avg train loss: 1.27773 - Sample: <<<*yud tteneverdle a oale* somen** *irn\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 26 - Avg train loss: 1.26475 - Sample: * undion in the to pavlen ig an hrufen* \n",
            "Epoch 27 - Avg train loss: 1.22684 - Sample: <<<<<<<<<<<<<<<* * anter ttee bef of cai\n",
            "Epoch 28 - Avg train loss: 1.21991 - Sample: <* yoy wiso to gveeor te mit the murgooe\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 29 - Avg train loss: 1.21289 - Sample: <<** deame aneul* om thit thiis *himg hi\n",
            "Epoch 30 - Avg train loss: 1.21374 - Sample: *ot * the te marss sony oxre tores undy \n",
            "Epoch 31 - Avg train loss: 1.20756 - Sample: <<<<<<<<<<<*houre af hol at ong the vrak\n",
            "Epoch 32 - Avg train loss: 1.19746 - Sample: <<<<*<e detrs hononl* lawhid the beack o\n",
            "Epoch 33 - Avg train loss: 1.17861 - Sample: <<*hoe terteam* fhe tte worg thalls toe \n",
            "Epoch 34 - Avg train loss: 1.17689 - Sample: *<nf usrye full bo falldd besy sorrttel*\n",
            "Epoch 35 - Avg train loss: 1.17118 - Sample: <<* *ay sale malrs hud and scisssor tho \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 36 - Avg train loss: 1.15716 - Sample: <<<<** cheaw for it *hivautt sibe fuct *\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 37 - Avg train loss: 1.14312 - Sample: **ouce me hald* net sho lmole* *ters rof\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 38 - Avg train loss: 1.15621 - Sample: <<<<<* deeay whe in moird wile who tern \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 39 - Avg train loss: 1.14055 - Sample: <<<<<<* onn* * elle iy ceke than wiy it \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 40 - Avg train loss: 1.14178 - Sample: <*sauve whaive if com therest heis and f\n",
            "Epoch 41 - Avg train loss: 1.13000 - Sample: <<<*fsroe artht u enlear at there* as nv\n",
            "Epoch 42 - Avg train loss: 1.11089 - Sample: <*t* the remy*o ame osm polne* *ey ance \n",
            "Epoch 43 - Avg train loss: 1.10807 - Sample: <<*ty tallurt* neshe of heans k rach spi\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 44 - Avg train loss: 1.12001 - Sample: <<* *ert the flours* * haws woitinc* * *\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 45 - Avg train loss: 1.10973 - Sample: <<<<<<<<<<<<<<<<<<<<<<* deam * * is the \n",
            "Epoch 46 - Avg train loss: 1.10104 - Sample: <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 47 - Avg train loss: 1.07437 - Sample: * yongearolqgill the gorcs to sho cealn*\n",
            "Epoch 48 - Avg train loss: 1.05975 - Sample: * heat ace mre whor who meds ying ay the\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 49 - Avg train loss: 1.05259 - Sample: *ow ou we *is il the of thing that tyite\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 50 - Avg train loss: 1.06156 - Sample: <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 51 - Avg train loss: 1.03654 - Sample: <<<<<<<<<<<<<* it * of the soas* *e antu\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 52 - Avg train loss: 1.03553 - Sample: <<<<<<<<<<* *aan *eyhi syeresdenttstevan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 53 - Avg train loss: 1.05686 - Sample: * ying *rur wur snomeying of thito wsees\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 54 - Avg train loss: 1.01073 - Sample: <<<<<<* dim* *ilon* ** * lofms lallssray\n",
            "Epoch 55 - Avg train loss: 1.01184 - Sample: <<* they hear hesring difhentr hasting w\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 56 - Avg train loss: 0.99891 - Sample: * * * this know day in tillisn* yay me*m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 57 - Avg train loss: 1.02307 - Sample: <<<*eato the mufsdiqou adiyest kromest e\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 58 - Avg train loss: 1.01888 - Sample: <<** *or thing mere fromsober cen farere\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 59 - Avg train loss: 1.00580 - Sample: <<<<* am the herests yide ourters osous*\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 60 - Avg train loss: 0.98690 - Sample: <<<<<<<<<<<<<<<<<<<<<<<*s * * to sheae o\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 61 - Avg train loss: 0.99073 - Sample: <* ans on whe geaes aly a lime* say a we\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 62 - Avg train loss: 0.97963 - Sample: <<<<<<<<<* hear thone and had and thouve\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 63 - Avg train loss: 0.98409 - Sample: * the whee old fans* of the gass as the \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 64 - Avg train loss: 0.97391 - Sample: <* *he wheat *an time watrrets forents o\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 65 - Avg train loss: 0.96091 - Sample: <<<<<<<*hy *e* * ve reacol* foin* *ae an\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 66 - Avg train loss: 0.96727 - Sample: <<<<<* am one exriying* *a*d to frose al\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 67 - Avg train loss: 0.95283 - Sample: * enming theterk of myte tte thote stiin\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 68 - Avg train loss: 0.94102 - Sample: <<*ot * lew whee lome teteind of thine w\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 69 - Avg train loss: 0.93402 - Sample: <<<** now the sathere mfing*d the scimfe\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 70 - Avg train loss: 0.93030 - Sample: <<<<<<<*roling tre thine whirite they sa\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 71 - Avg train loss: 0.92943 - Sample: **allonnoqee the ofing* tal to thy hanke\n",
            "Epoch 72 - Avg train loss: 0.91364 - Sample: <<<**ou thite**bon * any the birth othin\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 73 - Avg train loss: 0.93655 - Sample: <<*ow oeld on ewtar thou sauss eevore an\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 74 - Avg train loss: 0.92148 - Sample: <<<<**ope nkworing* *te tand of the gils\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 75 - Avg train loss: 0.91464 - Sample: * *hauw* dar therr ying hair on band for\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 76 - Avg train loss: 0.89888 - Sample: <<<<<<<<<**ledd you wit roawes all the g\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z61FW4lD6tVz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tlm.train_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m45cd8An9lAi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tlm.logs.keys()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZYGuFeC9lUe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.set_option('display.max_colwidth', -1)\n",
        "ex = pd.DataFrame(columns=['example', 'topic_num', 'epoch'], data=tlm.logs['examples'])\n",
        "ex.sample(50).sort_values('epoch')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFXBO8SjcVK6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(tlm.logs['val_loss']), len(tlm.logs['val_loss'][0]), len(tlm.logs['val_loss'][0][0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgnBZEQg9l74",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean_losses = [np.array(v).mean(axis=0) for v in tlm.logs['val_loss']]\n",
        "max_losses = [np.array(v).max(axis=0) for v in tlm.logs['val_loss']]\n",
        "min_losses = [np.array(v).min(axis=0) for v in tlm.logs['val_loss']]\n",
        "epoch = np.arange(len(mean_losses))\n",
        "\n",
        "fig, axs = plt.subplots(3)\n",
        "for k in range(3):\n",
        "  axs[k].plot(epoch, [m[k] for m in mean_losses], color='b')\n",
        "  axs[k].fill_between(epoch, [m[k] for m in max_losses], [m[k] for m in min_losses], color='b', alpha=0.3)\n",
        "  axs[k].set_xlabel('Epoch')\n",
        "  axs[k].set_ylabel('Loss: {}'.format(tlm.train_model.metrics_names[k]))\n",
        "\n",
        "fig.set_size_inches(12,12)\n",
        "plt.title('validation loss - max, mean, min')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIj0eHx79mLi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epoch = np.arange(len(tlm.logs['topic_entropy']))\n",
        "\n",
        "fig, ax_mean = plt.subplots()\n",
        "ax_mean.stackplot(epoch, np.array(tlm.logs['topic_fraction']).T)\n",
        "ax_mean.set_xlabel('Epoch')\n",
        "ax_mean.set_ylabel('Topic Balance')\n",
        "\n",
        "ax_ent = ax_mean.twinx()\n",
        "topic_entropy = [e[0] for e in tlm.logs['topic_entropy']]\n",
        "max_topic_entropy = [(-p*np.log(p)).sum() for p in tlm.logs['topic_fraction']]\n",
        "frac_topic_entropy = [e/m for e,m in zip(topic_entropy, max_topic_entropy)]\n",
        "ax_ent.plot(epoch, frac_topic_entropy, color='k')\n",
        "ax_ent.set_ylabel('Topic entropy/max topic entropy (for given topic balance)')\n",
        "ax_ent.set_ylim(0,1.05)\n",
        "\n",
        "fig.set_size_inches(12,6)\n",
        "fig.tight_layout()\n",
        "plt.title('topic balance estimates - max, mean, min')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8mm7fWNhaCg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}